\documentclass[11pt]{article}

\usepackage{spikey}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}

\title{Probabilistic Graphical Models}
\author{Tianyu Du}

\newcommand{\dsep}[0]{\text{d-sep}}
\newcommand{\pa}[0]{\text{Par}}

\begin{document}
	\maketitle
	\section{Graphical Representations}
	\subsection{Factors}
	\begin{definition}
		Let $X_1, X_2, \cdots, X_k$ be a set of random variables, then a \textbf{factor} $\phi$ is a mapping from values of these random variables to $\R$.
		\begin{align}
			\phi: Val(X_1, X_2, \cdots, X_k) \to \R
		\end{align}
		The set of random variables $\{X_1, X_2, \cdots, X_k\}$ is defined as the \textbf{scope} of $\phi$.
	\end{definition}
	
	\begin{definition}
		Let $\phi_1$ and $\phi_2$ be two factors with scopes $\{A,B\}$ and $\{B, C\}$.
		Then the \textbf{factor product} $\phi_1 \times \phi_2$ is a factor with scope $\{A, B, C\}$ defined as
		\begin{align}
			\phi_1 \cdot \phi_2 (a, b, c) = \phi_1(a, b) \cdot \phi_2(b, c)
		\end{align}
	\end{definition}
	
	\begin{definition}
		Let $\phi$ be a factor with scope $\{A, B, C\}$, then \textbf{marginalizing $C$ from $\phi$} results in a factor $\phi'$ with scope $\{A, B\}$ defined as the following:
		\begin{align}
			\phi'(a, b) = \sum_{c \in Val(C)} \phi(a, b, c)
		\end{align}
	\end{definition}
	
	\begin{definition}
		The \textbf{factor reduction} operation restricts $\phi(A,B,C)$ to take only a specific value of $C=c$, and results in a factor $\phi'$ with scope $\{A, B\}$.
		\begin{align}
			\phi'(a, b) = \phi(a, b, c)
		\end{align}
	\end{definition}
	
	\subsection{Semantics and Factorization}
	\begin{definition}
		A \textbf{Bayesian network} consists of (i) a directed acyclic graph (DAG) $G$ whose nodes correspond to random variables $X_1, \cdots, X_n$
		 (ii) and a conditional probability distribution $P(X_i|Par_G(X_i))$ for each node $X_i$. The joint distribution is defined as the factorization
		 \begin{align}
		 	P(X_1, \cdots, X_n) &= \prod_{i=1}^n P(X_i|\pa_G(X_i))
		 \end{align}
	\end{definition}
	
	\begin{definition}
		Let $G$ be a graph over $X_1, \cdots, X_n$, then the joint probability $P$ \textbf{factorizes} over $G$ if and only if
		\begin{align}
			P(X_1, \cdots, X_n) &= \prod_{i=1}^n P(X_i|\pa_G(X_i))
		\end{align}
	\end{definition}
	
	\subsection{Pass of Influences in Bayesian Networks}
	\begin{definition}
		A path $X_1 - \dots - X_k$ in Bayesian network $G$ is \textbf{active} if there is no explaining-away structure $X_{i-1} \rightarrow X_i \leftarrow X_{i+1}$ in it.
	\end{definition}
	
	\begin{definition}
		Let $Z \subseteq V_G$ be a set of random variables in the Bayesian network, then a path $X_1 - \dots - X_k$ in $G$ is \textbf{active conditioned on $Z$} if 
		\begin{enumerate}
			\item for all explaining-away structure $X_{i-1} \rightarrow X_i \leftarrow X_{i+1}$ in the path, $X_i$ or some decedents of $X_i$ are in $Z$,
			\item \ul{and} no other node in the path is in $Z$.
		\end{enumerate}
	\end{definition}
	
	\begin{definition}
		Let $X, Y, Z \subseteq V_G$, if there is no path from $X$ to $Y$ is active conditioned on $Z$, then $X$ and $Y$ are \textbf{d-separated} by $Z$ in graph $G$ denoted as $\dsep_G(X, Y|Z)$.
	\end{definition}
	
	\subsection{Independencies and Factorizations}
	\begin{definition}
		Let $X, Y, Z$ be random variables with distribution $P$, then $X \perp Y$ if and only if $P(X, Y) = P(X)P(Y)$, $X \perp Y | Z$ if and only if $P(X, Y|Z) = P(X|Z)P(Y|Z)$.
	\end{definition}
	
	\begin{proposition}
		Let $X, Y, Z$ be random variables with distribution $P$, then $X \perp Y$ if and only if $P(X,Y)$ factorizes as the following
		\begin{align}
			P(X, Y) \propto \phi_1(X) \phi_1(Y) \label{eq:1}
		\end{align}
		and $X \perp Y|Z$ if and only if $P(X, Y, Z)$ factorizes as
		\begin{align}
			P(X, Y, Z) \propto \phi_1(X, Z) \phi_1(Y, Z) \label{eq:2}
		\end{align}
		
		\begin{proof}
			Relation (\ref{eq:1}) follows the definition immediately.
			Suppose $X \perp Y|Z$, then
			\begin{align}
				P(X, Y | Z) &= P(X|Z)P(Y|Z) \\
				\iff P(X,Y,Z) &= P(X|Z)P(Y|Z)P(Z) \\
				P(X,Y,Z) &\propto P(X|Z)P(Z)P(Y|Z)P(Z) \\
				&= P(X,Z) P(Y,Z) \\
				&= \phi_1(X, Z) \phi_1(Y, Z)
			\end{align}
		\end{proof}
	\end{proposition}
	
%	\begin{proposition}
%		Generally, let $P$ be a distribution on random variables $X_1, \cdots, X_n$, then $X_i \perp X_j$ if and only if the factorization of $P$ contains $\phi_i(X_i)$ and $\phi_j(X_j)$.
%	\end{proposition}
	
	\begin{theorem}[Factorization$\implies$Independence]
		If $P$ factorizes over $G$, and $\dsep_G(X,Y|Z)$ then $P$ satisfies $(X\perp Y|Z)$.
%		\begin{proof}
%			Let $V$ denote the set of all random variables in the network and $V \backslash \{X, Y, Z\} = \{V_1, V_2, \cdots, V_n\}$.
%			Because $P$ factorizes over $G$,
%			\begin{align}
%				P(X, Y, Z) &= \sum_{V_i: i=1,\cdots, n} P(X, Y, Z, V_1, V_2, \cdots, V_n) \\
%				 &=\sum_{V_i: i=1,\cdots, n} P(X|\pa_G(X)) P(Y|\pa_G(Y)) P(Z|\pa_G(Z)) \prod_{i=1}^n P(V_i|\pa_G(V_i))
%			\end{align}
%		\end{proof}
	\end{theorem}
	
	\begin{theorem}
		For any random variable $X_i$ in the Bayesian network, $X_i$ is d-separated from all its non-descendants by $\pa_G(X_i)$.
	\end{theorem}
	
	\begin{corollary}
		If $P$ factorizes over $G$, then in $P$, any variable is independent of its non-descendants given its parents.
	\end{corollary}
	
	\begin{definition}
		Let $\mc{I}(G)$ denote the collection of independencies implicitly encoded by d-separations in graph $G$,
		\begin{align}
			\mc{I}(G) := \{(X\perp Y|Z): X, Y, Z \in V\ s.t.\ \dsep_G (X, Y | Z)\}
		\end{align}
		If a distribution $P$ over $V$ satisfies all independencies in $\mc{I}(G)$, then we say that $G$ is an \textbf{I-map} (independency map) of $P$.
		
		That is, the I-map of distribution $P$ is a graphical representation of all (and probably more) independencies of $P$.
	\end{definition}
	
	\begin{example}
		Let $P$ be a probability distribution and let $G$ be an I-map for $P$. Let $\mc{I}(P)$ and $\mc{I}(G)$ denote sets of independencies in $P$ and $G$.
		Suppose $G$ is a I-map of $P$, then all independencies encoded in $G$ are satisfied by $P$, therefore,
		\begin{align}
			\mc{I}(G) \subseteq \mc{I}(P)
		\end{align}
	\end{example}
	
	\begin{example}
		The I-map can be used for two graphs as well. $G_1$ is a I-map of $G_1$ if $\mc{I}(G_1) \subseteq \mc{I}(G_2)$.
		That is, $G_1$ is an I-map of $G_2$ if it does not make independence assumptions that are not true in $G_2$.
	\end{example}
	
	\begin{theorem}[Independence$\implies$Factorization]
		If $G$ is an I-map for $P$, that is, $P$ adheres all independencies encoded in $G$, then $P$ factorizes over $G$.
%		\begin{proof}
%			
%		\end{proof}
	\end{theorem}
	\subsection{Template Models}
	\begin{definition}
		A \textbf{template variable} $X(U_1, \cdots, U_k)$ is instantiated (duplicated) multiple times in a graph.
		\textbf{Template models} are languages that specify how ground variables (i.e., instantiations of template variables) inherit dependency model from template.
	\end{definition}
	
	\begin{notation}
		Let $X^{(t)}$ denote the variable at time $t \Delta$, where $\Delta$ is the time granularity in the discrete timeline.
		Let $X^{(t:t')} = \{X^{(t)}, ^{(t+1)}, \cdots, X^{(t')}\}$ denote the set of variables over a period of time.
	\end{notation}
	
	\begin{definition}
		A Bayesian network is said to satisfy the \textbf{Markov assumption} if
		\begin{align}
			X^{(t+1)} \perp X^{(0:t-1)} | X^{(t)}
		\end{align}
		When Markov assumption holds, we may express the joint distribution of all $X$ as
		\begin{align}
			P(X^{(0:T)}) = P(X^{(0)}) \prod_{t=0}^{T-1} P(X^{(t+1)}|X^{(t)})
		\end{align}
	\end{definition}
	
	\begin{definition}
		A series of random variables $X^{(0)}, X^{(1)}, \cdots, X^{(T)}$ satisfies the \textbf{time invariance} assumption if there exists a template probability model $P(X'|X)$ such that for all $t$,
		\begin{align}
			P(X^{(t+1)}|X^{(t)}) = P(X'|X)
		\end{align}
	\end{definition}
	
	\begin{definition}
		A \textbf{2-time-slice Bayesian network} (2TNB) over $X_1, \cdots, X_n$ ($n$ random variables for each time step) is specified as a Bayesian network fragment such that
		\begin{itemize}
			\item The nodes include $X_1', \cdots, X_n'$ and a subset of $X_1, \cdots, X_n$,
			\item and only the nodes $X_n', \cdots, X_n'$ have parents and a conditional probability distribution.
		\end{itemize}
		Further, the 2TBN defines a conditional distribution
		\begin{align}
			P(X'|X) = \prod_{i=1}^n P(X_i' | \pa(X_i'))
		\end{align}
	\end{definition}
	
	\begin{definition}
		A \textbf{dynamic Bayesian network} (DNB) over $X_1, \cdots, X_n$ is defined by
		\begin{itemize}
			\item a 2TNB, BN$_{\rightarrow}$, over $X_1, \cdots, X_n$,
			\item and a Bayesian network, BN$^{(0)}$, over $X_1^{(0)}, \cdots, X_n^{(0)}$.
		\end{itemize}
	\end{definition}
	
	\begin{definition}
		For a trajectory over $0, \cdots, T$, the \textbf{ground (unrolled) network} of a DNB is a model such that
		\begin{itemize}
			\item the dependency model for $X_1^{(0)}, \cdots, X_n^{(0)}$ is copied from BN$^{(0)}$,
			\item and the dependency model for $X_1^{(t)}, \cdots, X_n^{(t)}$ is copied from BN$_{\rightarrow}$.
		\end{itemize}
	\end{definition}
\end{document}





















