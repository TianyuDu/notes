\documentclass[11pt]{article}

\usepackage{spikey}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}

\usepackage{setspace}
\linespread{1.15}

\title{Lecture Notes \\ MS\&E: Causal Inferences (Autumn 2020) \\ @ Stanford University}

\newcommand{\science}[0]{\underline{\textbf{Y}}}
\newcommand{\tauhat}[0]{\hat{\tau}}

\author{Tianyu Du}

\begin{document}
	\maketitle
	\section{Potential Outcome Framework}
	\subsection{Rubin Causal Model / Neyman-Rubin Potential Causal Framework}
	\paragraph{Define assignments and potential outcomes}
	\begin{itemize}
		\item Let $i = 1, 2, \dots, N$ be indices of $N$ units (subjects).
		\item For simplicity, assume binary intervention $Z_i$ with value $z_i \in \{0, 1\}$ or \{control, treatment\}.
		\item Let vector of random variables $\textbf{Z} = (Z_1, Z_2, \dots, Z_N)$ be the population assignment. A realization of \textbf{Z}, $\textbf{z} = (z_1, z_2, \dots, z_N)$, denotes actual treatment assignments to the population.
		\item $Y_i(\textbf{z})$ denotes the potential outcome of unit $i$ when the entire population receives treatment \textbf{z}.
		\item A \textbf{z} defines an universe / realization of \textbf{Z}, therefore, there are $2^N$ potential outcomes for each unit $i$.
	\end{itemize}

	\begin{assumption}
		The Stable Unit Treatment Value Assumption (SUTVA).
		\begin{enumerate}
			\item No interference between units: outcome of unit $i$ is not affected by treatment of player $j$ for all $j \neq i$.
			\begin{align}
				\quad \textbf{z}_i = \textbf{z}'_i \implies Y_i(\textbf{z}) = Y_i(\textbf{z}')
			\end{align}
			With the first assumption holds, we can write the potential outcome of unit $i$ as a function of $z_i$ only: $Y_i(z_i)$.
			\item No hidden version of treatments: $Y_i(z_i)$ is a well-defined function.
			\begin{align}
				z_i = z_i' \implies Y_i(z_i) = Y(z_i')
			\end{align}
		\end{enumerate}
	\end{assumption}
	Given SUTVA, there are only 2 potential outcomes for each unit $i$.

	\paragraph{The Science} Denote the vector of potential outcomes $\textbf{Y}(0) := (Y_1(0), Y_2(0), \dots, Y_N(0))$ and $\textbf{Y}(1) := (Y_1(1), Y_2(1), \dots, Y_N(1))$.
	The science (aka. schedule of potential outcomes) is defined as
	\begin{align}
		\science := (\textbf{Y}(0), \textbf{Y}(1))
	\end{align}
	$\science$ tells outcomes in all factual and counterfactual scenarios.
	
	\begin{definition}
		An \textbf{assignment mechanism} is a distribution of \textbf{Z}. A generic mechanism is often denoted as $\textbf{Z} \sim \eta$.
		An assignment mechanism is a \textbf{randomized experiment} if
		\begin{enumerate}
			\item \emph{Probabilistic}: $0 < P(z_i|\textbf{X}, \textbf{Y}(0), \textbf{Y}(1)) < 1$.
			\item \emph{Known assignment mechanism}: the assignment can be expressed explicitly.
			\item \emph{Individualistic}: $P(z_i|\textbf{X}, \textbf{Y}(0), \textbf{Y}(1)) = P(z_i|X_i, Y_i(0), Y_i(1))$.
			\item \emph{Unconfoundness} $P(\textbf{z}|\textbf{X}, \textbf{Y}(0), \textbf{Y}(1)) = P(\textbf{z}|\textbf{X})$, where \textbf{X} is known and observed covariates.
		\end{enumerate}
	\end{definition}
	
	\begin{remark}
		The unconfoundness assumption cannot be tested empirically since we never have full knowledge on the science.
	\end{remark}
	
	\subsection{Neymanian Inference}
	\paragraph{Average Treatment Effect and Difference in Mean Estimator} 
	\begin{itemize}
		\item Define the \textbf{average treatment effect} (ATE) to be $\tau = \overline{\textbf{Y}(1)} - \overline{\textbf{Y}(0)}$.
		\item Let $\vez^{obs}$ and $\vey^{abs}$ denote the observed treatment assignment and outcome.
		\item We may construct the \textbf{difference in mean} estimator for ATE: $\hat{\tau}^{DIM} = \overline{\vey^{obs}(1)} - \overline{\vey^{obs}(0)}$.
		\item Given the $\science$, each realization of $\vez$ leads to one value of $\hat{\tau}$. The $\vez \sim \eta$ induces a distribution on $\hat{\tau} \sim P_\eta$.
	\end{itemize}
	
	\begin{definition}
		Given an assignment mechanism $\eta$, the \textbf{bias} of an estimator is
		\begin{align}
			Bias_\eta(\hat{\tau}, \tau, \science) = \expe_{\vez \sim \eta} (\hat{\tau}(\vez, \science)) - \tau(\science)
		\end{align}
		An estimator is \textbf{unbiased} for $\tau$ under design $\eta$ if for all $\science$, $Bias_\eta(\hat{\tau}, \tau, \science) = 0$.
	\end{definition}
	We can rewrite the observed outcome as
	\begin{align}
		y_i = z_i y_i(1) + (1-z_i) y_i(0)
	\end{align}
	
	\subsection{Causal Estimands}
	\paragraph{Clarification}
	\begin{itemize}
		\item Estimand: the quantity of interest to be estimated.
		\item Estimator: a procedure to approximate estimand from data.
	\end{itemize}
	
	\begin{example}
		Examples of causal estimands include
		\begin{itemize}
			\item Individual treatment effect: $\tau_i := y_i(1) - y_i(0)$.
			\item Average treatment effect: $\tau^{ATE} := \overline{\vey_i(1)} - \overline{\vey_i(0)}$.
			\item Conditional average treatment effect: let $X_i$ be the controlled co-variate, $\tau_x := \frac{1}{N_x} \sum_{i=1}^N \id{X_i = x} \tau_i$.
			\item Lift: $L := \frac{\overline{\vey_i(1)} - \overline{\vey_i(0)}}{\overline{\vey_i(0)}}$.
		\end{itemize}
	\end{example}

	\paragraph{Super-population Estimands} So far, we have fixed and finite population without a model. $\tau$ is specific to the population of size $N$. We may assume the $N$ units are actually i.i.d. samples from a super-population:
	\begin{align}
		(Y_i(0), Y_i(1)) \overset{i.i.d.}{\sim} P
	\end{align}
	with
	\begin{align}
		\expe_P(Y_i(0)) = \mu_0 \\
		\expe_P(Y_i(1)) = \mu_1
	\end{align}
	One super-population estimand is $\theta = \expe(\tau) = \mu_1 - \mu_0$, we may construct models to estimate parameters $\mu_0$ and $\mu_1$.
	
	\subsection{No Causation without Manipulation}
	
	\section{Randomized Experiments: Neyman v.s. Fisher Inferences}
	\subsection{The randomization based framework}
	\par Throughout this section, we assume SUTVA. However, the randomization removes the need for most assumptions beyond SUTVA.
	\paragraph{Reasoned-Basis for Inference}
	\begin{itemize}
		\item Recall: observed $y_i = z_i y_i(1) + (1-z_i) y_i(0)$.
		\item Classical statistics:
		\begin{enumerate}
			\item Assume $y_i|(z_i=1) \sim \mc{N}(\mu_1, \sigma_1^2),\ y_i|(z_i=0) \sim \mc{N}(\mu_0, \sigma_0^2)$.
			\item Compute MLE $\hat{\mu}_1^{MLE}$ and $\hat{\mu}_0^{MLE}$.
			\item Estimate $\hat{\tau}^{MLE} = \hat{\mu}_1^{MLE} - \hat{\mu}_0^{MLE}$.
		\end{enumerate}
		\item Randomization-based inference:
		\begin{enumerate}
			\item Consider \science as fixed, but unobserved.
			\item \vez is a random variable.
			\item \vey(\vez) is the observed realization from a function of \vez. (recall: \vez defines \vey through $\eta$.)
			\item No other assumptions.
			\item All randomness came from \vez, the distribution of $\vez \sim P_\eta(\vez)$ will play a crucial role.
		\end{enumerate}
	\end{itemize}
	
	\paragraph{Assignment Mechanism}
	\begin{example}[Bernoulli Assignment]
		Let $p \in (0, 1)$, $P(z_i = 1) = p$, $P(\vez) = \prod_{i=1}^N p^{z_i} (1-p)^{1-z_i}$.
	\end{example}
	
	\begin{example}[Completely Randomized Design (CRD)]
		CRD($N_1$, $N$) randomly draws $N_1$ units from $N$ units and assigns them treatment.
		\begin{align}
			P(z_i=1) &= \frac{N_1}{N} \\
			P(\vez) &= \begin{cases}
				{N \choose N_1}^{-1} &\tx{ if } \sum z_i = N_1 \\
				0 &\tx{ otherwise}
			\end{cases}
		\end{align}
	\end{example}
	
	\begin{proposition}
		Let $\tau = \tau^{ATE}$ and $\hat{\tau} = \hat{\tau}^{DIM}$, suppose $\eta = CRD(N_1, N)$ with $0 < N_1 < N$, then the difference-in-mean estimator is unbiased. That is, for all \science, $Bias_\eta(\hat{\tau}, \tau; \science) = 0$.
		\begin{proof}
			\begin{align}
				\hat{\tau}^{DIM} &\equiv \frac{1}{\sum z_i} \sum_{i=1}^N z_i y_i - \frac{1}{\sum (1-z_i)} \sum_{i=1}^N (1-z_i) y_i \\
				&= \frac{1}{N_1} \sum_{i=1}^N z_i y_i - \frac{1}{N-N_1} \sum_{i=1}^N (1-z_i) y_i \tx{ by CRD} \\
				\implies \expe_\eta [\hat{\tau}^{DIM}] &= \frac{1}{N_1} \sum_{i=1}^N \expe_\eta[z_i y_i] - \frac{1}{N-N_1} \sum_{i=1}^N \expe_\eta[(1-z_i) y_i] \\
				&= \frac{1}{N_1} \sum_{i=1}^N \expe_\eta[z_i y_i(1)] - \frac{1}{N-N_1} \sum_{i=1}^N \expe_\eta[(1-z_i) y_i(0)] \\
				&= \frac{1}{N_1} \sum_{i=1}^N y_i(1) \expe_\eta[z_i] - \frac{1}{N-N_1} \sum_{i=1}^N  y_i(0) \expe_\eta[(1-z_i)] \\
				&= \frac{1}{N_1} \sum_{i=1}^N y_i(1) \frac{N_1}{N} - \frac{1}{N-N_1} \sum_{i=1}^N  y_i(0) \frac{N-N_1}{N} \\
				&= \frac{1}{N} \sum_{i=1}^N y_i(1) - y_i(0)
			\end{align}
		\end{proof}
	\end{proposition}
	
	\begin{definition}
		The variance of estimator $\hat{\tau}$ is defined as
		\begin{align}
			Var_\eta(\hat{\tau}) := \expe_\eta [(\hat{\tau} - \expe_\eta[\hat{\tau}])^2]
		\end{align}
	\end{definition}
	
	\begin{proposition}
		Let $\tau = \tau^{ATE}$ and $\hat{\tau} = \hat{\tau}^{DIM}$, suppose $\eta = CRD(N_1, N)$ with $0 < N_1 < N$, define $N_0 = N - N_1$. Then,
		\begin{align}
			Var_\eta(\hat{\tau}) = \frac{V_1}{N_1} + \frac{V_0}{N_0} - \frac{V_{1,0}}{N}
		\end{align}
		where $V_a$ is the variance of potential outcome $a$ and $V_{1,0}$ is the variance of treatment effect.
		\begin{align}
			V_{a}&=\frac{1}{N-1} \sum_i \left(y_{i}(a)-\bar{y}(a)\right)^2\ a \in \{0, 1\} \\
			V_{1,0} &= \frac{1}{N-1} \sum_{i}\left(\tau_{i}-\tau\right)^2
		\end{align}
	\end{proposition}
	
	\subsection{Inferences}
	\begin{itemize}
		\item Suppose we have a \ul{normal approximation} is large samples:
		\begin{align}
			\frac{\hat{\tau} - \tau}{\sigma(\hat{\tau})} \sim \mc{N}(0, 1)	
		\end{align}
		Then,
		\begin{align}
			CI_{1-\alpha} = [\tauhat - q_{1-\alpha/2} \sigma(\tauhat), \tauhat + q_{1-\alpha/2} \sigma(\tauhat)]
		\end{align}
		The confidence interval satisfies
		\begin{align}
			P_\eta (\tau \in CI_{1-\alpha}) \approx 1 - \alpha
		\end{align}
		\item However, $v$ and $\sigma \equiv \sqrt{v}$ depends on unknown quantities (true variances of potential outcomes and treatment effects). We need to estimate $\hat{v}$ using data.
		\item In general, we wish construct a \ul{conservative} estimation $\hat{v}$ such that $\expe[\hat{v}] \geq v$, which leads to a larger confidence interval satisfying $P_\eta(\tau \in \hat{CI}_{1-\alpha}) \geq 1 - \alpha$.
		\item Specifically, we can use conventional sample estimations for $V_1$ and $V_0$ while ignoring the variance of treatment effects. Doing so leads to a conservative estimation of variance.
	\end{itemize}
	
	\begin{proposition}
		The Neyman estimator of variance is
		\begin{align}
			\hat{v} = \frac{\hat{V_1}}{N_1} + \frac{\hat{V_0}}{N_0}
		\end{align}
		where
		\begin{align}
			\hat{v}_{1}&=\frac{1}{N_{1}-1} \sum_{i=1}^N z_{i}\left(y_{i}^{obs}-\overline{y}^{obs}\right)^2 \\
			\hat{v}_{0}&=\frac{1}{N_{0}-1} \sum_{i=1}^N (1-z_{i})\left(y_{i}^{obs}-\overline{y}^{obs}\right)^2
		\end{align}
		under $CRD(N_1, N_0)$,
		\begin{align}
			\expe_\eta[\hat{v}] \geq Var_\eta(\tauhat)
		\end{align}
	\end{proposition}
	\subsection{Hypothesis Testing: As a Stochastic Proof by Contradiction}
	\begin{itemize}
		\item $H_0: \overline{Y}(1) = \overline{Y}(0)$ (i.e., $\tau^{ATE} = 0$).
		\item Define $T^{obs} = \frac{\tauhat-0}{\sqrt{\hat{v}}} = \frac{\tauhat}{\sqrt{\hat{v}}}$.
		\item Define the $p$-value as $1 - \Phi(T^{obs})$ (one-sided) or $2 (1 - \Phi(T^{obs}))$ (two-sided).
		\item Then,
		\begin{align}
			P_\eta(p \leq \alpha | H_0) \leq \alpha
		\end{align}
		\item That is, we firstly suppose $H_0$ to be true, in this case, $T^{obs}$ should follow $\mc{N}(0, 1)$ (the null distribution). If we observe some $T^{obs}$ that is unlikely under the null distribution, that is, $T^{obs}$ contradicts the null distribution, we reject $H_0$.
	\end{itemize}
	
	\subsection{Horvitz-Thompson Estimator for $\tau^{ATE}$}
	\begin{definition}
		Let $\eta$ be any design that is a randomized experiment, let $\Pi_i = P_\eta(Z_i=1)$ and $0 < \Pi_i < 1$. Define
		\begin{align}
			\tauhat^{HT} = \frac{1}{N} \sum_{i=1}^N \frac{z_i}{\Pi_i}y_i + \frac{1}{N} \sum_{i=1}^N \frac{1-z_i}{1-\Pi_i}y_i
		\end{align}
		Note that $\tauhat^{HT}$ is a special case of \ul{inverse propensity-score weighting} (IPW) estimators.
	\end{definition}
	
	\begin{proposition}
		Let $\eta$ be any design that is a randomized experiment, then the HT estimator $\tauhat^{HT}$ is \ul{unbiased} for $\tau^{ATE}$.
		\begin{proof}
			\begin{align}
				\expe_\eta \tauhat^{HT} &= \frac{1}{N} \sum_{i=1}^N \expe_\eta [\frac{z_i}{\Pi_i}y_i] + \frac{1}{N} \sum_{i=1}^N \expe_\eta[\frac{1-z_i}{1-\Pi_i}y_i] \\
				&= \frac{1}{N} \sum_{i=1}^N \expe_\eta [\frac{z_i}{\Pi_i}y_i(1)] + \frac{1}{N} \sum_{i=1}^N \expe_\eta[\frac{1-z_i}{1-\Pi_i}y_i(0)] \\
				&= \frac{1}{N} \sum_{i=1}^N \expe_\eta[z_i] \frac{1}{\Pi_i}y_i(1) + \frac{1}{N} \sum_{i=1}^N \expe_\eta[1-z_i] \frac{1}{1-\Pi_i}y_i(0) \\
				&= \frac{1}{N} \sum_{i=1}^N y_i(1) + \frac{1}{N} \sum_{i=1}^N y_i(0) \\
				&= \tau^{ATE}
			\end{align}
		\end{proof}
	\end{proposition}
\end{document}






















